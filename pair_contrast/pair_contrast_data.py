import json
import torch
import random
import itertools

from torch.utils.data import Dataset
from dataclasses import dataclass

from transformers import PegasusTokenizerFast, PegasusForConditionalGeneration


def load_jsonl(filename):
    records = []
    with open(filename, 'r') as file:
        for line in file:
            records.append(json.loads(line))
    return records


class ContrastiveDataset(Dataset):
    # TODO: include reference summary in the collection of "positive summaries"
    def __init__(self, filename, tokenizer, max_src_len=512, max_tgt_len=128, source_prefix='',
                 use_original_ref=True, n_contrast_pair=3):
        self.data = load_jsonl(filename)
        self.src_max_len = max_src_len
        self.tgt_max_len = max_tgt_len
        self.tokenizer = tokenizer
        self.use_original_ref = use_original_ref
        self.n_contrast_pair = n_contrast_pair

        # for each dialogue, we have a reference summary, a list of (positive_summary, negative_summary) pairs.
        # positive summaries and negative summaries are generated by ChatGPT
        self.dataset = []
        cur_instance = 0
        for record in self.data:
            dialogue = source_prefix + record['dialogue']
            tokenized_source = tokenizer(dialogue, truncation=True, max_length=max_src_len)
            tokenized_source = {'input_ids': tokenized_source.input_ids,
                                'attention_mask': tokenized_source.attention_mask}
            ref_summ = record['reference_summary']
            augmented_summaries = record['summaries']
            pos_summaries = [x['pos'] for x in augmented_summaries]
            neg_summaries = [x['neg'] for x in augmented_summaries]

            with tokenizer.as_target_tokenizer():
                tokenized_pos_summs = tokenizer(pos_summaries, truncation=True, max_length=max_tgt_len)['input_ids']
                tokenized_neg_summs = tokenizer(neg_summaries, truncation=True, max_length=max_tgt_len)['input_ids']
                tokenized_ref = tokenizer(ref_summ, truncation=True, max_length=max_tgt_len)['input_ids']

            n_pos = len(pos_summaries)
            if self.use_original_ref:
                # the last element in 'pos_samples' will be used as target for cross entropy loss.
                # here each positive summary has a chance to act as the target for cross entropy loss.
                # the original human-written reference summary is always included in the positive pool
                for i in range(n_pos):
                    pos_samples = tokenized_pos_summs[:i] + tokenized_pos_summs[i+1:] + [tokenized_ref]
                    if self.n_contrast_pair - 1 > 0:
                        pos_samples = random.sample(pos_samples, self.n_contrast_pair - 1) + [tokenized_pos_summs[i]]

                    else:
                        pos_samples = [tokenized_pos_summs[i]]
                    neg_samples = random.sample(tokenized_neg_summs, self.n_contrast_pair)

                    self.dataset.append({
                        'source': tokenized_source,
                        # 'pos_samples': tokenized_pos_summs + [tokenized_ref],
                        'pos_samples': pos_samples,
                        'neg_samples': neg_samples,
                    })
                    cur_instance += 1

                if self.n_contrast_pair - 1 > 0:
                    pos_samples = random.sample(tokenized_pos_summs, self.n_contrast_pair - 1)
                else:
                    pos_samples = []
                self.dataset.append({
                    'source': tokenized_source,
                    'pos_samples': pos_samples + [tokenized_ref],
                    'neg_samples': random.sample(tokenized_neg_summs, self.n_contrast_pair),
                })
                cur_instance += 1
            else:
                # the last element in 'pos_samples' will be used as target for cross entropy loss
                # only use ChatGPT generated positive summaries as target for cross entropy loss
                # also exclude the original human-written reference summary from positive pool
                for i in range(n_pos):
                    self.dataset.append({
                        'source': tokenized_source,
                        'pos_samples': tokenized_pos_summs[:i] + tokenized_pos_summs[i+1:] + [tokenized_pos_summs[i]],
                        'neg_samples': tokenized_neg_summs
                    })

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        record = self.dataset[idx]
        return record


@dataclass
class DataCollatorForContrastive:
    """
    Data collator that will dynamically pad the inputs received, as well as the labels.
    Args:
        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):
            The tokenizer used for encoding the data.
        model (:class:`~transformers.PreTrainedModel`):
            The model that is being trained. If set and has the `prepare_decoder_input_ids_from_labels`, use it to
            prepare the `decoder_input_ids`
            This is useful when using `label_smoothing` to avoid calculating loss twice.
        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):
            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
            among:
            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
              sequence is provided).
            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
              maximum acceptable input length for the model if that argument is not provided.
            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
              different lengths).
        max_length (:obj:`int`, `optional`):
            Maximum length of the returned list and optionally padding length (see above).
        pad_to_multiple_of (:obj:`int`, `optional`):
            If set will pad the sequence to a multiple of the provided value.
            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
            7.5 (Volta).
        label_pad_token_id (:obj:`int`, `optional`, defaults to -100):
            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).
    """

    tokenizer: PegasusTokenizerFast
    model: PegasusForConditionalGeneration = None
    padding = True
    max_length = None
    pad_to_multiple_of = None
    label_pad_token_id = -100

    def __call__(self, features):
        sources = []  # input_dialogues, len(sources)=batch_size

        # pos + neg summaries, len(targets)=pos+neg. In our case, 3 negative + 4 positive for each dialogue
        targets = []

        # for example, [0, 0, 1, 1, 1]
        # means the first two sequences are corresponding to the first source sequence (input dialogue)
        # the 3rd, 4th and 5th sequences are corresponding to the second source sequence (input dialogue)
        src_select_indices = []  # len(src_select_indices) = len(targets)

        positive_contrast_pos = []
        valid_contrast_pos = []

        # the indices of reference summaries for cross entropy loss. len=batch_size (one reference for each sample)
        cross_entropy_pos = []

        accumulate = 0
        for i, feature in enumerate(features):
            valid_i = []  # stores the indices of positive samples
            valid_j = []  # stores the indices of both positive and negative samples

            source = feature['source']  # {input_ids, attention_mask}
            sources.append(source)

            for pos_target in feature['pos_samples']:
                src_select_indices.append(i)

                targets.append({'labels': pos_target})

                valid_i.append(accumulate)
                valid_j.append(accumulate)  # put the positive samples first
                accumulate += 1

            # Mark the last positive summary as the reference for cross entropy loss
            cross_entropy_pos.append(accumulate - 1)

            # generate combinations of positive samples
            comb = list(itertools.combinations(valid_i, 2))
            positive_contrast_pos.extend(comb)

            for neg_target in feature['neg_samples']:
                src_select_indices.append(i)

                targets.append({'labels': neg_target})

                valid_j.append(accumulate)  # then put the negative samples
                accumulate += 1

            # Generate combinations of valid samples (positive + negative)
            comb = list(itertools.combinations(valid_j, 2))
            valid_contrast_pos.extend(comb)

        positive_contrast = torch.zeros((len(targets), len(targets)), dtype=torch.bool)
        for aaa, bbb in positive_contrast_pos:
            positive_contrast[aaa, bbb] = True
            positive_contrast[bbb, aaa] = True

        valid_contrast = torch.zeros((len(targets), len(targets)), dtype=torch.bool)
        for aaa, bbb in valid_contrast_pos:
            valid_contrast[aaa, bbb] = True
            valid_contrast[bbb, aaa] = True

        for i in range(len(targets)):
            valid_contrast[i, i] = False
            positive_contrast[i, i] = False

        source_features = sources  # 'input_ids', 'attention_mask', both size=(batch_size, seq_len)
        target_features = targets  # 'labels', size=(14, seq_len). (4 pos + 3 neg) * batch_size = 14

        labels = [feature["labels"] for feature in target_features] if "labels" in target_features[0].keys() else None
        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the
        # same length to return tensors.
        if labels is not None:
            max_label_length = max(len(l) for l in labels)
            padding_side = self.tokenizer.padding_side
            for feature in target_features:
                remainder = [self.label_pad_token_id] * (max_label_length - len(feature["labels"]))
                feature["labels"] = (
                    feature["labels"] + remainder if padding_side == "right" else remainder + feature["labels"]
                )

        source_features = self.tokenizer.pad(
            source_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )

        labels = torch.tensor([feature['labels'] for feature in target_features], dtype=torch.long)

        # prepare decoder_input_ids
        decoder_input_ids = None
        if self.model is not None and hasattr(self.model, "prepare_decoder_input_ids_from_labels"):
            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=labels)

        # positive_contrast is a bool matrix for contrastive loss computation. 14 x 14 matrix.
        # positive_contrast[i,j]=True means it is a pair of interest (a positive pair), otherwise False
        # used to calculate numerator of contrastive loss and normalizing constant
        # (the number of positive pairs in total)

        # valid_contrast has the same shape of positive_contrast but contains pairwise relations between both
        # positive pairs and negative pairs. Used to calculate denominator of contrastive loss
        all_features = {
            'input_ids': source_features['input_ids'],  # size=(batch_size, seq_len)
            'attention_mask': source_features['attention_mask'],  # size=(batch_size, seq_len)
            'labels': labels,  # size=(14, label_seq_len)  14 is just an example here
            'decoder_input_ids': decoder_input_ids,  # same size as 'labels'. Shift one position to the right of 'labels'
            'src_select_indices': torch.tensor(src_select_indices, dtype=torch.long),  # size=14
            'ce_pos': torch.tensor(cross_entropy_pos, dtype=torch.long),  # size=2
            'positive_contrast': positive_contrast,
            'valid_contrast': valid_contrast
        }

        return all_features