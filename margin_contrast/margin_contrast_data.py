import json
from dataclasses import dataclass

import torch
from torch.utils.data import Dataset
from transformers import PegasusTokenizerFast, PegasusForConditionalGeneration


def load_jsonl(filename):
    records = []
    with open(filename, 'r') as file:
        for line in file:
            records.append(json.loads(line))
    return records


class ContrastiveMultiDataset(Dataset):
    def __init__(self, filename, tokenizer, max_src_len=512, max_tgt_len=128,
                 source_prefix='', policy="set-wise margin contrast"):
        self.data = load_jsonl(filename)
        self.src_max_len = max_src_len
        self.tgt_max_len = max_tgt_len
        self.tokenizer = tokenizer
        self.policy = policy

        # we have a reference summary, a positive summary (generated by ChatGPT) and a
        # negative summary (generated by ChatGPT) for each dialogue
        self.dataset = []
        for record in self.data:
            dialogue = source_prefix + record['dialogue']
            tokenized_source = tokenizer(dialogue, truncation=True, max_length=max_src_len)
            tokenized_source = {'input_ids': tokenized_source.input_ids,
                                'attention_mask': tokenized_source.attention_mask}
            ref_summ = record['reference_summary']
            augmented_summaries = record['summaries']

            # we have a set of positive summaries and a set of negative summaries, but they are not paired
            # for example, pos_summaries[0] and neg_summaries[0] is a direct pair with minimal modification.
            # but we don't use this information
            if self.policy == "setwise":
                pos_summaries = [x['pos'] for x in augmented_summaries]
                neg_summaries = [x['neg'] for x in augmented_summaries]

                with tokenizer.as_target_tokenizer():
                    tokenized_pos_summs = tokenizer(pos_summaries, truncation=True, max_length=max_tgt_len)['input_ids']
                    tokenized_neg_summs = tokenizer(neg_summaries, truncation=True, max_length=max_tgt_len)['input_ids']
                    tokenized_ref = tokenizer(ref_summ, truncation=True, max_length=max_tgt_len)['input_ids']

                positive_summs = tokenized_pos_summs + [tokenized_ref]
                for i in range(len(pos_summaries) + 1):
                    self.dataset.append({
                        'source': tokenized_source,
                        # put the original reference summary into the positive pool
                        'positive_summaries': positive_summs,
                        'negative_summaries': tokenized_neg_summs,
                        'index_of_ref_summ': i,
                        'reference_summary': positive_summs[i]
                    })
            elif self.policy == "pairwise":
                pos_summaries = [""] * len(augmented_summaries)
                neg_summaries = [""] * len(augmented_summaries)
                contrast_pairs = []  # an element (0, 0) means pos_summaries[0] and neg_summaries[0] is a contrast pair

                for i, x in enumerate(augmented_summaries):
                    if 'pos' in x and 'neg' in x:  # it means we have a direct contrast pair
                        if len(x['pos']) > 10 and len(x['neg']) > 10:
                            pos_summaries[i] = x['pos']
                            neg_summaries[i] = x['neg']
                            contrast_pairs.append((i, i))

                if len(contrast_pairs) < 1:
                    continue

                with tokenizer.as_target_tokenizer():
                    tokenized_pos_summaries = tokenizer(pos_summaries, truncation=True, max_length=max_tgt_len)['input_ids']
                    tokenized_neg_summaries = tokenizer(neg_summaries, truncation=True, max_length=max_tgt_len)['input_ids']
                    tokenized_ref_summ = tokenizer(ref_summ, truncation=True, max_length=max_tgt_len)['input_ids']

                # for each positive summary, we use it as the label for standard cross entropy, and we
                # use another contrast pair for contrastive loss
                pos_summaries = pos_summaries + [ref_summ]
                tokenized_pos_summaries = tokenized_pos_summaries + [tokenized_ref_summ]
                # print(f"#pos summs: {len(tokenized_pos_summaries)}")
                # print(f"#neg summs: {len(tokenized_neg_summaries)}")
                for j in range(len(pos_summaries)):
                    if len(pos_summaries[j]) < 10:
                        continue

                    # find all contrast pairs, excluding pairs containing the current positive summary itself
                    # print(f"current pos: {j}")
                    valid_contrast_pairs = [(x, x) for (x, x) in contrast_pairs if x != j]
                    # print(f"all contrast pairs: {contrast_pairs}")
                    # print(f"valid contrast pairs: {valid_contrast_pairs}")
                    if len(valid_contrast_pairs) < 1:
                        continue
                    for pair in valid_contrast_pairs:
                        pos_idx, neg_idx = pair
                        # print(f"contrast pair: ({pos_idx}, {neg_idx})")
                        self.dataset.append({
                            'source': tokenized_source,
                            # the first element is used for contrastive loss, the second is the target for cross entropy
                            'positive_summaries': [tokenized_pos_summaries[pos_idx], tokenized_pos_summaries[j]],
                            'index_of_ref_summ': 1,  # index of the cross entropy target in positive_summaries
                            'negative_summaries': [tokenized_neg_summaries[neg_idx]],
                            'reference_summary': tokenized_pos_summaries[j],
                            'pos_summs_text': [pos_summaries[pos_idx], pos_summaries[j]],  # for correctness check
                            'neg_summs_text': [neg_summaries[neg_idx]]  # for correctness check
                        })

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        record = self.dataset[idx]
        return record


@dataclass
class DataCollatorForContrastive:
    """
    Data collator that will dynamically pad the inputs received, as well as the labels.
    Args:
        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):
            The tokenizer used for encoding the data.
        model (:class:`~transformers.PreTrainedModel`):
            The model that is being trained. If set and has the `prepare_decoder_input_ids_from_labels`, use it to
            prepare the `decoder_input_ids`
            This is useful when using `label_smoothing` to avoid calculating loss twice.
        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):
            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
            among:
            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
              sequence is provided).
            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
              maximum acceptable input length for the model if that argument is not provided.
            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
              different lengths).
        max_length (:obj:`int`, `optional`):
            Maximum length of the returned list and optionally padding length (see above).
        pad_to_multiple_of (:obj:`int`, `optional`):
            If set will pad the sequence to a multiple of the provided value.
            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
            7.5 (Volta).
        label_pad_token_id (:obj:`int`, `optional`, defaults to -100):
            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).
    """
    tokenizer: PegasusTokenizerFast
    model: PegasusForConditionalGeneration = None
    padding = True
    max_length = None
    pad_to_multiple_of = None
    label_pad_token_id = -100

    def __call__(self, features):
        sources = []
        targets = []
        src_select_indices = []  # record the corresponding index of the source in this batch, for each target sequence
        pos_neg = []  # 1: positive target, 0: negative target
        ref_summ_idx = []  # 1: reference target, 0: otherwise
        for i, record in enumerate(features):
            sources.append(record["source"])
            for j, pos_tgt_input_id in enumerate(record["positive_summaries"]):
                src_select_indices.append(i)
                targets.append({'labels': pos_tgt_input_id})
                pos_neg.append(1)
                if j == record["index_of_ref_summ"]:
                    ref_summ_idx.append(1)
                else:
                    ref_summ_idx.append(0)
            for neg_tgt_input_id in record["negative_summaries"]:
                src_select_indices.append(i)
                targets.append({'labels': neg_tgt_input_id})
                pos_neg.append(0)
                ref_summ_idx.append(0)

        labels = [feature["labels"] for feature in targets] if "labels" in targets[0].keys() else None

        # pad labels using label_pad_token_id, so cross entropy loss function will ignore the padded positions
        if labels is not None:
            max_label_length = max(len(l) for l in labels)
            padding_side = self.tokenizer.padding_side
            for feature in targets:
                remainder = [self.label_pad_token_id] * (max_label_length - len(feature["labels"]))
                feature["labels"] = (
                    feature["labels"] + remainder if padding_side == "right" else remainder + feature["labels"]
                )
        labels = torch.tensor([feature['labels'] for feature in targets], dtype=torch.long)
        # print(f"==> labels size: {labels.size()}")

        source_features = self.tokenizer.pad(
            sources,  # {input_ids, attention_mask}
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt"
        )

        # prepare decoder_input_ids
        decoder_input_ids = None
        if self.model is not None and hasattr(self.model, "prepare_decoder_input_ids_from_labels"):
            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=labels)

        all_features = {
            'input_ids': source_features['input_ids'],  # batch_size x seq_len
            'attention_mask': source_features['attention_mask'],  # batch_size x seq_len
            'labels': labels,
            'decoder_input_ids': decoder_input_ids,
            'src_select_indices': torch.tensor(src_select_indices, dtype=torch.long),
            'pos_neg': torch.tensor(pos_neg, dtype=torch.long),
            'ref_summ_idx': torch.tensor(ref_summ_idx, dtype=torch.long)
        }
        return all_features
